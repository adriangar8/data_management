{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark module to locate Spark installation\n",
    "import findspark\n",
    "\n",
    "# Initialize Spark installation path\n",
    "findspark.init(\"/home/alumno/Escritorio/spark/spark-3.2.2-bin-hadoop2.7\")\n",
    "\n",
    "# Import SparkContext and SparkConf modules from PySpark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create a SparkConf object with the app name and master URL\n",
    "conf=SparkConf().setAppName(\"intro\").setMaster(\"local\")\n",
    "\n",
    "# Create a SparkContext object with the SparkConf object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Import SparkSession and functions modules from PySpark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession object with the SparkContext object\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. -Data analysis with Spark DataFrames: loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the movies.csv file into a Spark DataFrame\n",
    "movies_df = spark.read.option(\"inferSchema\",\"true\").option(\"header\", \"true\").csv(\"ml-latest-small/movies.csv\")\n",
    "\n",
    "# Read the ratings.csv file into a Spark DataFrame\n",
    "ratings_df = spark.read.option(\"inferSchema\",\"true\").option(\"header\", \"true\").csv(\"ml-latest-small/ratings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. -Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|           title|              genres|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|      1|   610|   5.0|1479542900|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   609|   3.0| 847221025|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   608|   2.5|1117408267|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   607|   4.0| 964744033|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   606|   2.5|1349082950|Toy Story (1995)|Adventure|Animati...|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the ratings_df and movies_df dataframes on the \"movieId\" column using a right join\n",
    "# This will create a new dataframe with all the movies and their ratings \n",
    "movie_ratings = ratings_df.join(movies_df, [\"movieId\"], \"right\")\n",
    "\n",
    "# Show the first 5 rows of the new dataframe\n",
    "movie_ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100854"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "- Can you obtain a basic summary list of statistics for our new movie ratings dataframe? Interesting information is the count, mean, max, and some selected percentiles. For each question of the tutorial, you must provide the following information:\n",
    "\n",
    "\n",
    "- What command are you going to use? Why?\n",
    "\n",
    "\n",
    "\n",
    "- Which is your Spark operation to solve the question?\n",
    "\n",
    "\n",
    "\n",
    "- Which output is providing your Spark command (3 lines max.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the movie_ratings dataframe by \"movieId\" column\n",
    "movie_grouped = movie_ratings.groupBy(\"movieId\")\n",
    "\n",
    "# Aggregate the grouped dataframe to calculate total ratings, mean rating, and max rating for each movie\n",
    "movie_stats = movie_grouped.agg(\n",
    "        F.count(F.col(\"rating\")).alias(\"total_ratings\"), \n",
    "        F.mean(F.col(\"rating\")).alias(\"mean_rating\"), \n",
    "        F.max(F.col(\"rating\")).alias(\"max_rating\"),\n",
    "\n",
    ")\n",
    "\n",
    "# Collect the statistics for each movie into a list\n",
    "stats_collected = movie_stats.collect()\n",
    "\n",
    "# Calculate the 25th, 50th, and 75th percentiles of the \"rating\" column in the movie_ratings dataframe\n",
    "percentiles = movie_ratings.stat.approxQuantile(\"rating\", [0.25, 0.5, 0.75], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+----------+\n",
      "|movieId|total_ratings|mean_rating|max_rating|\n",
      "+-------+-------------+-----------+----------+\n",
      "|    148|            1|        5.0|       5.0|\n",
      "|    471|           40|       3.55|       5.0|\n",
      "|    496|            1|        5.0|       5.0|\n",
      "+-------+-------------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "25th percentile: 3.0\n",
      "50th percentile (median): 3.5\n",
      "75th percentile: 4.0\n"
     ]
    }
   ],
   "source": [
    "movie_stats.show(3)\n",
    "\n",
    "print(\"25th percentile:\", percentiles[0])\n",
    "print(\"50th percentile (median):\", percentiles[1])\n",
    "print(\"75th percentile:\", percentiles[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First off, to understand how viewers feel about different movies, we can organize the data by each film and summarize it. We do this by grouping all the ratings for the same movie together and then calculating the total number of ratings it's received, the average rating, and the highest rating it's gotten. This is done through a process called 'group by' and 'aggregate,' which is just a fancy way of saying we're pooling together all the information by movie and then doing some quick math to get the numbers we want.*\n",
    "\n",
    "*The reason we use these particular methods is because they're built for handling large amounts of data efficiently. Especially when we're curious about how ratings spread out — like what's a low rating, a typical rating, or a high rating — we use something called 'approximate quantiles.' This is a shortcut that helps us estimate these specific points in our data without getting bogged down in details, making it a lot faster when dealing with lots of data.*\n",
    "\n",
    "*As for what we end up with, imagine a neat table where each row shows a movie's ID, how many times it's been rated, what its average rating is, and the highest rating it's received. That's what our first method produces. The second part, with the 'approximate quantiles,' gives us a simple list of numbers that represent typical rating milestones, like what rating is low-end, middle-of-the-road, and on the higher side for all the movies together.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method randomSplit in module pyspark.sql.dataframe:\n",
      "\n",
      "randomSplit(weights, seed=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    weights : list\n",
      "        list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "        Weights will be normalized if they don't sum up to 1.0.\n",
      "    seed : int, optional\n",
      "        The seed for sampling.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      "    >>> splits[0].count()\n",
      "    2\n",
      "    \n",
      "    >>> splits[1].count()\n",
      "    2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100854"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the help function on the randomSplit method of the movie_ratings dataframe\n",
    "# This will display the documentation for the randomSplit method\n",
    "help(movie_ratings.randomSplit)\n",
    "\n",
    "# Split the movie_ratings dataframe into two dataframes for training and validation\n",
    "# The split is done randomly with 80% of the data going to the training dataframe and 20% going to the validation dataframe\n",
    "# The second argument of the randomSplit method is a seed value for the random number generator\n",
    "(train_df, v_df) = movie_ratings.randomSplit([0.8, 0.2], 0)\n",
    "\n",
    "# Count the number of rows in the movie_ratings dataframe\n",
    "movie_ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20354"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the validation dataframe with the train dataframe on the \"userId\" column using a left semi join\n",
    "# This will create a new dataframe with only the records that have a matching \"userId\" in both dataframes\n",
    "# Then join the resulting dataframe with the train dataframe on the \"movieId\" column using a left semi join\n",
    "# This will create a new dataframe with only the records that have a matching \"movieId\" in both dataframes\n",
    "validation_df = ( v_df\n",
    "                 .join(train_df, [\"userId\"], \"left_semi\")\n",
    "                 .join(train_df, [\"movieId\"], \"left_semi\")\n",
    ")\n",
    "\n",
    "# Join the validation dataframe with the validation dataframe and keep only the records that do not have a matching \"movieId\" and \"userId\" in both dataframes\n",
    "# This will create a new dataframe with only the records that are in the validation dataframe but not in the validation_df dataframe\n",
    "non_matching_recs = v_df.join(validation_df, [\"movieId\", \"userId\"], \"left_anti\")\n",
    "\n",
    "# Add the non-matching records to the train dataframe\n",
    "train_df = train_df.union(non_matching_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "\n",
    "- What kind of join operations are used in left semi and left anti? Can you explain these operations with our validation example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *A left semi join acts as a filter, keeping only the rows from the left DataFrame that have corresponding matches in the right DataFrame. It's like an intersection, but it only brings the columns from the left side. When applied to our DataFrames, the left semi join on userId then on movieId between v_df and train_df returns only those records from v_df where the userId and movieId also exist in train_df, essentially creating a validation set that only includes users and movies that the model has already seen during training.*\n",
    "\n",
    "2. *On the flip side, a left anti join finds the rows in the left DataFrame that do not have matching rows in the right DataFrame. It’s the opposite of a left semi join – a set difference. For our DataFrames, when v_df is left anti joined with validation_df, it produces non_matching_recs, a set of records that are unique to your validation set, meaning these user-movie pairs were not included in the training set. This allows us to add back any unique combinations to train_df, ensuring our training set is as comprehensive as possible.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**\n",
    "\n",
    "- Train_df has now more or less records than initially? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After performing the operations described, train_df would have more records than it initially had. This increase is because of the addition of non_matching_recs to train_df using the union operation. The non_matching_recs DataFrame contains records from v_df that did not have a corresponding user and movie pair in train_df (as determined by the left anti join). By uniting train_df with non_matching_recs, we are adding unique user-movie pairs from the validation set into the training set, which were not there before. This process ensures that the training set includes all possible user-movie interactions, potentially improving the model's ability to generalize from the training data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. -Managing columns of validation datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, mean, split, explode, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|    193|   243|   5.0| 837155377|    Showgirls (1995)|               Drama|\n",
      "|  44191|   540|   5.0|1179108778|V for Vendetta (2...|Action|Sci-Fi|Thr...|\n",
      "|    296|   540|   5.0|1179108599| Pulp Fiction (1994)|Comedy|Crime|Dram...|\n",
      "|     62|   243|   5.0| 837155394|Mr. Holland's Opu...|               Drama|\n",
      "|   1221|   392|   5.0|1027524082|Godfather: Part I...|         Crime|Drama|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_df.sort(col(\"rating\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:**\n",
    "\n",
    "- Create a new DF derived from train_df grouping all records with the same rating, count them, and sort by the rating column in descending order. The expected output of this transformation should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|rating|count|\n",
      "+------+-----+\n",
      "|   5.0|10632|\n",
      "|   4.5| 6856|\n",
      "|   4.0|21725|\n",
      "|   3.5|10610|\n",
      "|   3.0|16100|\n",
      "|   2.5| 4486|\n",
      "|   2.0| 6055|\n",
      "|   1.5| 1465|\n",
      "|   1.0| 2308|\n",
      "|   0.5| 1082|\n",
      "|  null|   18|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the train_df dataframe by \"rating\" column and count the number of records in each group\n",
    "ratings_distribution = train_df.groupBy(\"rating\").count()\n",
    "\n",
    "# Sort the ratings_distribution dataframe by \"rating\" column in descending order\n",
    "ratings_distribution = ratings_distribution.orderBy(F.desc(\"rating\"))\n",
    "\n",
    "# Display the ratings_distribution dataframe\n",
    "ratings_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To summarize the records in the DataFrame by their ratings and then arrange them in descending order, we'll employ a sequence of specific Spark DataFrame operations. First, we'll use the groupBy function on the rating column. This function clusters the DataFrame into groups based on the unique rating values.*\n",
    "\n",
    "*After grouping the data by rating, we'll apply the count function. This aggregation function calculates the number of occurrences for each distinct rating in the DataFrame. It's a standard approach to quantify the frequency of each rating value within the dataset.*\n",
    "\n",
    "*Finally, to present the data in descending order based on the rating values, we'll utilize the orderBy function in conjunction with the desc (descending) method. This operation sorts the aggregated counts so that the higher ratings appear first. This sequence of operations—groupBy, followed by count, and then orderBy with desc—effectively produces the final DataFrame, which will list the count of each rating from highest to lowest.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+----------------+-------------------------------------------+-------------------------------------------------+\n",
      "|movieId|userId|rating|timestamp |title           |genres                                     |genres_array                                     |\n",
      "+-------+------+------+----------+----------------+-------------------------------------------+-------------------------------------------------+\n",
      "|1      |1     |4.0   |964982703 |Toy Story (1995)|Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "|1      |5     |4.0   |847434962 |Toy Story (1995)|Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "|1      |7     |4.5   |1106635946|Toy Story (1995)|Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "|1      |15    |2.5   |1510577970|Toy Story (1995)|Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "|1      |17    |4.5   |1305696483|Toy Story (1995)|Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "+-------+------+------+----------+----------------+-------------------------------------------+-------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the \"genres\" column of the train_df dataframe by \"|\" character and create a new column \"genres_array\" with the resulting array\n",
    "train_with_genres_array = train_df.withColumn(\"genres_array\", split(\"genres\", \"\\|\"))\n",
    "\n",
    "# Display the first 5 rows of the resulting dataframe\n",
    "train_with_genres_array.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------------------------------------------+-------------------------------------------------+---------+\n",
      "|movieId|userId|rating|genres                                     |genres_array                                     |genre    |\n",
      "+-------+------+------+-------------------------------------------+-------------------------------------------------+---------+\n",
      "|1      |1     |4.0   |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Adventure|\n",
      "|1      |1     |4.0   |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Animation|\n",
      "|1      |1     |4.0   |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Children |\n",
      "|1      |1     |4.0   |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Comedy   |\n",
      "|1      |1     |4.0   |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Fantasy  |\n",
      "+-------+------+------+-------------------------------------------+-------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the columns \"movieId\", \"userId\", \"rating\", \"genres\", and \"genres_array\" from the train_with_genres_array dataframe\n",
    "# Then, explode the \"genres_array\" column into multiple rows, one for each genre in the array, and create a new column \"genre\" with the resulting genre value\n",
    "train_with_genres_exploded = (\n",
    "    train_with_genres_array\n",
    "    .select(\"movieId\", \"userId\", \"rating\", \"genres\", \"genres_array\")\n",
    "    .withColumn(\"genre\", explode(\"genres_array\"))\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the resulting dataframe\n",
    "train_with_genres_exploded.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|genre      |genre_rating      |\n",
      "+-----------+------------------+\n",
      "|Crime      |3.6657157871030703|\n",
      "|Romance    |3.5048340647284695|\n",
      "|Thriller   |3.4936604449472095|\n",
      "|Adventure  |3.507378740970072 |\n",
      "|Drama      |3.6545317265271073|\n",
      "|War        |3.7982522796352582|\n",
      "|Documentary|3.780632411067194 |\n",
      "|Fantasy    |3.4920134510298446|\n",
      "|Mystery    |3.630674448767834 |\n",
      "|Musical    |3.570934776074542 |\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the train_with_genres_exploded dataframe by \"genre\" column and calculate the mean of the \"rating\" column for each group\n",
    "# Rename the resulting column to \"genre_rating\"\n",
    "mean_genre_rating = (\n",
    "    train_with_genres_exploded\n",
    "        .groupBy(\"genre\")\n",
    "        .agg(mean(col(\"rating\")).alias(\"genre_rating\"))\n",
    ")\n",
    "\n",
    "# Display the first 10 rows of the resulting dataframe\n",
    "mean_genre_rating.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------+\n",
      "|genre      |genre_rating      |num_movies|\n",
      "+-----------+------------------+----------+\n",
      "|Crime      |3.6657157871030703|13324     |\n",
      "|Romance    |3.5048340647284695|14589     |\n",
      "|Thriller   |3.4936604449472095|21221     |\n",
      "|Adventure  |3.507378740970072 |19381     |\n",
      "|Drama      |3.6545317265271073|33785     |\n",
      "|War        |3.7982522796352582|3949      |\n",
      "|Documentary|3.780632411067194 |1014      |\n",
      "|Fantasy    |3.4920134510298446|9517      |\n",
      "|Mystery    |3.630674448767834 |6168      |\n",
      "|Musical    |3.570934776074542 |3328      |\n",
      "+-----------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the train_with_genres_exploded dataframe by \"genre\" column and calculate the mean of the \"rating\" column for each group\n",
    "# Also, count the number of unique movies in each group\n",
    "# Rename the resulting columns to \"genre_rating\" and \"num_movies\"\n",
    "mean_genre_rating_movies = (\n",
    "    train_with_genres_exploded\n",
    "        .groupBy(\"genre\")\n",
    "        .agg(\n",
    "            mean(col(\"rating\")).alias(\"genre_rating\"),\n",
    "            countDistinct(\"movieId\").alias(\"num_movies\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Display the first 10 rows of the resulting dataframe\n",
    "mean_genre_rating_movies.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------+\n",
      "|genre      |genre_rating      |num_movies|\n",
      "+-----------+------------------+----------+\n",
      "|Crime      |3.6657157871030703|1199      |\n",
      "|Romance    |3.5048340647284695|1596      |\n",
      "|Thriller   |3.4936604449472095|1894      |\n",
      "|Adventure  |3.507378740970072 |1263      |\n",
      "|Drama      |3.6545317265271073|4361      |\n",
      "|War        |3.7982522796352582|382       |\n",
      "|Documentary|3.780632411067194 |440       |\n",
      "|Fantasy    |3.4920134510298446|779       |\n",
      "|Mystery    |3.630674448767834 |573       |\n",
      "|Musical    |3.570934776074542 |334       |\n",
      "+-----------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the train_with_genres_exploded dataframe by \"genre\" column and calculate the mean of the \"rating\" column for each group\n",
    "# Also, count the number of unique movies in each group\n",
    "# Rename the resulting columns to \"genre_rating\" and \"num_movies\"\n",
    "mean_genre_rating_movies = (\n",
    "    train_with_genres_exploded\n",
    "        .groupBy(\"genre\")\n",
    "        .agg(\n",
    "            mean(col(\"rating\")).alias(\"genre_rating\"),\n",
    "            countDistinct(\"movieId\").alias(\"num_movies\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Display the first 10 rows of the resulting dataframe\n",
    "mean_genre_rating_movies.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:**\n",
    "\n",
    "Extend the previous DataFrame to have a new column with the unique number of ratings for each movie. You need to consider a countDistinct with both \"movieId\" and \"userId\" so that a user only ranks\n",
    "once for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------+-----------+\n",
      "|   genre|      genre_rating|num_movies|num_ratings|\n",
      "+--------+------------------+----------+-----------+\n",
      "|   Crime|3.6657157871030703|      1199|      13321|\n",
      "| Romance|3.5048340647284695|      1596|      14584|\n",
      "|Thriller|3.4936604449472095|      1894|      21216|\n",
      "+--------+------------------+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the train_with_genres_exploded dataframe by \"genre\" column and calculate the mean of the \"rating\" column for each group\n",
    "# Also, count the number of unique movies in each group and the number of unique ratings for each movie\n",
    "# Rename the resulting columns to \"genre_rating\", \"num_movies\", and \"num_ratings\"\n",
    "mean_ratings = (\n",
    "    train_with_genres_exploded\n",
    "    .groupBy(\"genre\")\n",
    "    .agg(\n",
    "        mean(col(\"rating\")).alias(\"genre_rating\"),\n",
    "        countDistinct(\"movieId\").alias(\"num_movies\"),\n",
    "        countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first 3 rows of the resulting dataframe\n",
    "mean_ratings.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This command will group the data by the genre column and calculate three aggregated metrics:*\n",
    "\n",
    "*The average rating for the genre (genre_rating).*\n",
    "*The number of unique movies in the genre (num_movies).*\n",
    "*The number of unique ratings for each movie within the genre (num_ratings).*\n",
    "\n",
    "*The countDistinct function takes two columns as arguments, in this case userId and movieId, to ensure that the count is based on unique pairs of users and movies. This means that if a user has rated the same movie multiple times, it will only count once.*\n",
    "\n",
    "*The groupBy followed by agg is the Spark operation used to solve this question. It allows us to compute multiple aggregate functions at once after grouping the data, which is efficient and concise for this type of computation.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** \n",
    "\n",
    "Can you program a top 10 list of best average rating genres? and a top 10 list of genres with most ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------+-----------+\n",
      "|      genre|      genre_rating|num_movies|num_ratings|\n",
      "+-----------+------------------+----------+-----------+\n",
      "|  Film-Noir|3.9236804564907275|        87|        701|\n",
      "|        War|3.7982522796352582|       382|       3948|\n",
      "|Documentary| 3.780632411067194|       440|       1012|\n",
      "|      Crime|3.6657157871030703|      1199|      13321|\n",
      "|      Drama|3.6545317265271073|      4361|      33773|\n",
      "|    Mystery| 3.630674448767834|       573|       6168|\n",
      "|  Animation| 3.627240461401952|       611|       5635|\n",
      "|       IMAX|  3.62106682649086|       158|       3337|\n",
      "|    Western|3.6017983301220293|       167|       1557|\n",
      "|    Musical| 3.570934776074542|       334|       3327|\n",
      "+-----------+------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code creates a new dataframe called top10_avg_rating_genres that contains the top 10 genres with the highest average rating.\n",
    "# It does this by sorting the mean_ratings dataframe by the \"genre_rating\" column in descending order and then limiting the output to the top 10 rows.\n",
    "# Finally, it displays the resulting dataframe using the show() method.\n",
    "top10_avg_rating_genres = (\n",
    "    mean_ratings\n",
    "    .orderBy(col(\"genre_rating\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top10_avg_rating_genres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+----------+-----------+\n",
      "|    genre|      genre_rating|num_movies|num_ratings|\n",
      "+---------+------------------+----------+-----------+\n",
      "|    Drama|3.6545317265271073|      4361|      33773|\n",
      "|   Comedy| 3.386513679933981|      3756|      31506|\n",
      "|   Action|3.4520333468889794|      1828|      24590|\n",
      "| Thriller|3.4936604449472095|      1894|      21216|\n",
      "|Adventure| 3.507378740970072|      1263|      19380|\n",
      "|  Romance|3.5048340647284695|      1596|      14584|\n",
      "|   Sci-Fi|3.4512548962715797|       980|      13786|\n",
      "|    Crime|3.6657157871030703|      1199|      13321|\n",
      "|  Fantasy|3.4920134510298446|       779|       9516|\n",
      "| Children|3.4114681365958592|       664|       7438|\n",
      "+---------+------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code creates a new dataframe called top10_most_rating_genres that contains the top 10 genres with the most ratings.\n",
    "# It does this by sorting the mean_ratings dataframe by the \"num_ratings\" column in descending order and then limiting the output to the top 10 rows.\n",
    "# Finally, it displays the resulting dataframe using the show() method.\n",
    "top10_most_rating_genres = (\n",
    "    mean_ratings\n",
    "    .orderBy(col(\"num_ratings\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top10_most_rating_genres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we want to create a top 10 list of genres by best average rating, we would start by grouping the data by the 'genre' field. We would then aggregate this grouped data to calculate the average rating for each genre using the avg function. After calculating the averages, we would order the results in descending order based on the average rating. To get only the top 10 genres, we would apply the limit function with a parameter of 10.*\n",
    "\n",
    "*We would follow a similar approach for obtaining the top 10 genres by the most ratings. First, we would group the data by 'genre'. Then, instead of averaging, we would count the number of ratings each genre has received using the count function. We would sort these counts in descending order to get the genres with the most ratings first. Finally, we would use the limit function again to get the top 10 results.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
